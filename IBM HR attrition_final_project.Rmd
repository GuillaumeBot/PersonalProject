---
title: "IBM's employee resignation predictor"
author: "Guillaume BOTHIER, Cissy MARTINEZ, Ziad MIKHAEL, Amir SALAMEH"
date: "25th May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("D:/INSEAD/Academics/P3/Data science/INSEADAnalytics-master/AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')

# Run below only once, then comment out
# New versions of the networkD3 package may not work properly, so install the following version
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/networkD3/networkD3_0.2.13.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
```
<hr>\clearpage

## Business issue description
Attracting and retaining the talent within a company is a very strong concern for companies. <br />
Using the following data set, we would like to understand which factors are relevant when keeping an employee onboard. <br />

<hr>\clearpage

## Analysis of the data set
The data has been downloaded from Kaggle: https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/data0. <br />

It consists of 1470 rows and 35 columns. <br />
Let us first have a look at the strucutre of the data and clean it a bit. <br /> 

We remove the columns that are useless: <br /> 

* EmployeeCount: all are 1 <br />
* Over18 : Yes for everyone <br />
* StandardHours : everyone was 80 <br />

We also converted the "Attrition"" parameter from "Yes" to "1" and "No" to 0 and renamed it AttritionDummy.<br />


```{r}
data.file.name = "D:/INSEAD/Academics/P3/Data science/Assignment/Final_assignment/IBM_HR_raw_data.csv"
ProjectData <- read.csv(data.file.name)
#ProjectData <- dplyr::tbl_df(ProjectData) #transfor the csm into a table
ProjectData = ProjectData %>% mutate(AttritionDummy = if_else(Attrition == "Yes",
                                              1,
                                              0))
ProjectData$AttritionDummy <- as.factor(ProjectData$AttritionDummy)
names(ProjectData)[1]='Age'
ProjectData = ProjectData %>% dplyr::select(-EmployeeCount, -Over18, -StandardHours, -Attrition)

names(ProjectData)[1]='Age'
# ProjectData = dplyr::rename(ProjectData, Age = Ã¯..Age) #fix a typo in the "Age" name

#ProjectData <- data.matrix(ProjectData) 


# Please ENTER the original raw attributes to use. 
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used = c(1,3:32) # FOR IBM cleaned data set


# Please ENTER the selection criteria for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "manual"

# Please ENTER the desired minumum variance explained 
# (Only used in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (Only used in case "manual" is the factor selection criterion used).
#manual_numb_factors_used = 12
manual_numb_factors_used = 8

# Please enter the maximum number of observations to show in the report and slides. 
# DEFAULT is 10. If the number is large the report may be slow.
max_data_report = 10

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Default is "varimax"
rotation_used = "varimax"
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData[,factor_attributes_used]
ProjectDataFactor <- data.matrix(ProjectDataFactor)
```

## Check the Data 



```{r, echo=FALSE, message = FALSE}
library(kableExtra)
kable(ProjectData) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>% scroll_box(width = "900px", height = "200px")
```
<hr>\clearpage

The data we use here have the following descriptive statistics: 

```{r}
iprint.df(round(my_summary(ProjectDataFactor), 2))
```
<hr>\clearpage

## Initial brainstorming

* We could try to predict attrition by spliting the data set into training and testing. We are going to use a **logistic regression** tool: **glm** <br />
* We could plot a correlation matrix to identify highly correlated factors <br />
* We could use CTREE to make sens of the main important factor <br />
* We could try to find clusters in the data set (unsupervised learning), look for patterns and adapt our HR strategy accordingly <br />

## Correlation matrix
```{r, echo=FALSE, message=FALSE}
#iprint.df(round(thecor,2), scale=TRUE)
#thecor <-as.matrix(thecor)
# From the web ================================
library(corrplot)

#corrplot.mixed(thecor, order="hclust", tl.col="black")
library(GGally)
ggcorr(ProjectData, nbreaks=8, palette='RdGy', label=TRUE, label_size=2.5, label_color='white', layout.exp = 10, legend.position = "left", label_alpha = TRUE, hjust = 1, size = 3, color = "grey50")

```

<hr>\clearpage




## Initial visualisation of the data set
The first step is to visualize the data and try to have a feeling of where it goes <br />
1 = YES : the employee left the company <br />
0 = NO : the employee hasn't left the company (yet!) <br />

```{r, echo=FALSE}
suppressMessages(library(ggplot2))
# devtools::install_github("karthik/rdrop2")
# library(ggplot2)
# library(rdrop2)
par(mfrow=c(1,2))
#ProjectData = as.data.frame(ProjectData)
ggplot(ProjectData, 
            aes(x = MonthlyIncome, fill = AttritionDummy)) + 
            geom_density(alpha = 0.7) + 
            scale_fill_manual(values = c("#386cb0","#fdb462"))

ggplot(ProjectData, 
            aes(x = DistanceFromHome, fill = AttritionDummy)) + 
            geom_density(alpha = 0.7) + 
            scale_fill_manual(values = c("#386cb0","#fdb462"))

ggplot(ProjectData, 
            aes(x = OverTime, fill = AttritionDummy)) + 
            geom_density(alpha = 0.7) + 
            scale_fill_manual(values = c("#386cb0","#fdb462"))


ggplot(ProjectData, 
            aes(x = RelationshipSatisfaction, fill = AttritionDummy)) + 
            geom_density(alpha = 0.7) + 
            scale_fill_manual(values = c("#386cb0","#fdb462"))

```
<hr>\clearpage

## CTREE model
We use CTREE model to try to make sense of the coefficients first.

```{r, echo= FALSE, message=FALSE }
if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")} # Check if you have universal installer package, install if not

pacman::p_load("caTools","caret","partykit","ROCR","lift","rpart","e1071")

#loading the data
HRData<-read.csv(data.file.name, header=TRUE, sep=",")
lookup <- c("No" = 0, "Yes" = 1)
HRData$Attrition <- lookup[HRData$Attrition]
#HRData$Attrition = as.numeric(HRData$Attrition)
str(HRData)

#splitting the data to train and test
set.seed(101) 
sample = sample.split(HRData, SplitRatio = .75)
trainHRData = subset(HRData, sample == TRUE)
testHRData  = subset(HRData, sample == FALSE)


ctreeModel1_new<-ctree(Attrition ~ .,data=trainHRData) #Run ctree on training data


#prediction using ctree
ctree_prediction_new <- predict(ctreeModel1_new,newdata=testHRData, type="response") #Predict classification (for confusion matrix); default with ctree

# CTREE Model Prediction
ctree_pred_testing <- prediction(ctree_prediction_new, testHRData$Attrition) #Calculate errors
ctree_ROC_testing <- performance(ctree_pred_testing,"tpr","fpr") #Create ROC curve data
plot(ctree_ROC_testing,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1)) #Plot ROC curve
auc.tmp <- performance(ctree_pred_testing,"auc") #Create AUC data
ctree_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
ctree_auc_testing #Display AUC value

plot(ctreeModel1_new)

table(testHRData$Attrition,ctree_prediction_new > 0.4)

```
<hr>\clearpage


The Accuracy of the model is 80.69%, we think we can do better!

## Logistic model
In this section we build a logistic model.

```{r , echo=TRUE}
pacman::p_load("caret","ROCR","lift","glmnet","MASS","e1071") #Check, and if needed install the necessary packages


#set a random number generation seed to ensure that the split is the same everytime
#set.seed(77850) 
#set.seed(87700) 
set.seed(2009) 
inTrain <- createDataPartition(y = ProjectData$AttritionDummy ,
                               p = 1029/1470, list = FALSE) #1029 gives a 70% split

IBM.training <- ProjectData[ inTrain,]
IBM.testing <- ProjectData[ -inTrain,]

# Regression model using all the data
# Transform 

fit<-glm(AttritionDummy~., data=IBM.training, family = binomial(link = 'logit'))

# Refining the previous model by finding the optimal AIC value
fit_AIC<-stepAIC(fit,direction = c("both"),trace = 0) #AIC stepwise want to maximize AIC

fit_AIC
plot(fit_AIC) #Error plots: similar nature to lm plots

## ---- Prediction analysis --------------
#Comparison of our prediction with the testing sample
AttritionDummy.probability<-predict(fit_AIC,newdata=IBM.testing,type="response")
logistic_classification<-rep("1",0)
logistic_classification[AttritionDummy.probability<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Retained.in.2012. == "1"))

logistic_classification <- as.factor(logistic_classification)
confusionMatrix(logistic_classification,IBM.testing$AttritionDummy) #Display confusion matrix

#ROC Curve
logistic_ROC_prediction <- prediction(AttritionDummy.probability, IBM.testing$AttritionDummy)
logistic_ROC <- performance(logistic_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(logistic_ROC) #Plot ROC curve

####AUC (area under curve)
auc.tmp <- performance(logistic_ROC_prediction,"auc") #Create AUC data
logistic_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
logistic_auc_testing #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value

```

<hr>\clearpage

### Conclusion for logistic model

* The AUC is around 83% in our various sensitivity analysis <br />
* The Accuracy is 89.38%, but the sensitivity = 1 raises question <br />
* 2 additional tests using different random seedvalue (with the same proportion 70% in training / 30% in testing), gave the following Accuracies:  86.92% and 87.23%. The accuracy looks constantly above 80%

<hr>\clearpage

## Next iteration
* In the next iteration, we will try to improve the results by engineering new features

<hr>\clearpage











# Second loop : data cleaning 
## Remove the data redundant
Based on the correlation matrix, we decided to remove the following data: <br />
* JobLevel is perfectly correlated with MonthlyIncome, we decide to keep MonthlyIncome <br />
* YearsAtCompany is highly correlated with YearsInCurrentRole (0.8) and YearsInCurrentRole is less correlated to other variables --> we keep it and remove YearsAtCompany (which wasn't in the glm model anyway) < br />
* Creating three new variables: z-score of Monthly salary per Job role ; z-score of age in a specific job role (are you a high potential) and a dummy that indicates ifnthenjob is generic (easy to switch company) < br/>
* The correlation matrix indicates that those three new variables aren't highly correlated with the oldest < br/>

```{r}
data.file.name = "D:/INSEAD/Academics/P3/Data science/Assignment/Final_assignment/IBM_HR_Data_engineered.csv"
ProjectData <- read.csv(data.file.name, sep=";" )
#ProjectData <- dplyr::tbl_df(ProjectData) #transfor the csm into a table
ProjectData = ProjectData %>% mutate(AttritionDummy = if_else(Attrition == "Yes",
                                              1,
                                              0))
ProjectData$AttritionDummy <- as.factor(ProjectData$AttritionDummy)
names(ProjectData)[1]='Age'
ProjectData = ProjectData %>% dplyr::select(-EmployeeCount, -Over18, -StandardHours, -Attrition, -YearsAtCompany, -JobLevel)

names(ProjectData)[1]='Age'
ProjectData$Generic.role <- as.factor(ProjectData$Generic.role )
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData[,factor_attributes_used]
ProjectDataFactor <- data.matrix(ProjectDataFactor)
```
<hr>\clearpage

## Correlation matrix
```{r, echo=FALSE, message=FALSE}
#iprint.df(round(thecor,2), scale=TRUE)
#thecor <-as.matrix(thecor)
# From the web ================================
library(corrplot)

#corrplot.mixed(thecor, order="hclust", tl.col="black")
library(GGally)
ggcorr(ProjectData, nbreaks=8, palette='RdGy', label=TRUE, label_size=2.5, label_color='white', layout.exp = 10, legend.position = "left", label_alpha = TRUE, hjust = 1, size = 3, color = "grey50")

```

<hr>\clearpage


## CTREE model number 2
We use CTREE model to try to make sense of the coefficients first.

```{r, echo= FALSE, message=FALSE }
if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")} # Check if you have universal installer package, install if not

pacman::p_load("caTools","caret","partykit","ROCR","lift","rpart","e1071")

#loading the data
HRData<-read.csv(data.file.name, header=TRUE, sep=";")
lookup <- c("No" = 0, "Yes" = 1)
HRData$Attrition <- lookup[HRData$Attrition]
#HRData$Attrition = as.numeric(HRData$Attrition)
str(HRData)

#splitting the data to train and test
set.seed(101) 
sample = sample.split(HRData, SplitRatio = .75)
trainHRData = subset(HRData, sample == TRUE)
testHRData  = subset(HRData, sample == FALSE)


ctreeModel1_new<-ctree(Attrition ~ .,data=trainHRData) #Run ctree on training data


#prediction using ctree
ctree_prediction_new <- predict(ctreeModel1_new,newdata=testHRData, type="response") #Predict classification (for confusion matrix); default with ctree

# CTREE Model Prediction
ctree_pred_testing <- prediction(ctree_prediction_new, testHRData$Attrition) #Calculate errors
ctree_ROC_testing <- performance(ctree_pred_testing,"tpr","fpr") #Create ROC curve data
plot(ctree_ROC_testing,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1)) #Plot ROC curve
auc.tmp <- performance(ctree_pred_testing,"auc") #Create AUC data
ctree_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
ctree_auc_testing #Display AUC value

plot(ctreeModel1_new)

table(testHRData$Attrition,ctree_prediction_new > 0.4)

```
<hr>\clearpage


The Accuracy of the model is 80.69%, we think we can do better!

## Logistic model number 2
In this section we build a logistic model.

```{r , echo=TRUE}
pacman::p_load("caret","ROCR","lift","glmnet","MASS","e1071") #Check, and if needed install the necessary packages


#set a random number generation seed to ensure that the split is the same everytime
#set.seed(77850) 
#set.seed(87700) 
set.seed(2009) 
inTrain <- createDataPartition(y = ProjectData$AttritionDummy ,
                               p = 1029/1470, list = FALSE) #1029 gives a 70% split

IBM.training <- ProjectData[ inTrain,]
IBM.testing <- ProjectData[ -inTrain,]

# Regression model using all the data
# Transform 

fit<-glm(AttritionDummy~., data=IBM.training, family = binomial(link = 'logit'))

# Refining the previous model by finding the optimal AIC value
fit_AIC<-stepAIC(fit,direction = c("both"),trace = 0) #AIC stepwise want to maximize AIC

fit_AIC
plot(fit_AIC) #Error plots: similar nature to lm plots

## ---- Prediction analysis --------------
#Comparison of our prediction with the testing sample
AttritionDummy.probability<-predict(fit_AIC,newdata=IBM.testing,type="response")
logistic_classification<-rep("1",0)
logistic_classification[AttritionDummy.probability<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Retained.in.2012. == "1"))

logistic_classification <- as.factor(logistic_classification)
confusionMatrix(logistic_classification,IBM.testing$AttritionDummy) #Display confusion matrix

#ROC Curve
logistic_ROC_prediction <- prediction(AttritionDummy.probability, IBM.testing$AttritionDummy)
logistic_ROC <- performance(logistic_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(logistic_ROC) #Plot ROC curve

####AUC (area under curve)
auc.tmp <- performance(logistic_ROC_prediction,"auc") #Create AUC data
logistic_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
logistic_auc_testing #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value

```

<hr>\clearpage

### Conclusion for logistic model 2

* The new engineered variables improved slightly the CTREE prediction however not significantly
* The glm AUC is around 83% in our various sensitivity analysis no change <br />
* The feature engineering didn't really improve the model.

<hr>\clearpage